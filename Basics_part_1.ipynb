{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# pip install -U spacy\n",
        "# python -m spacy download en_core_web_sm\n",
        "import spacy\n",
        "\n",
        "# Load English tokenizer, tagger, parser and NER\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Process whole documents\n",
        "text = (\"When Sebastian Thrun started working on self-driving cars at \"\n",
        "        \"Google in 2007, few people outside of the company took him \"\n",
        "        \"seriously. ‚ÄúI can tell you very senior CEOs of major American \"\n",
        "        \"car companies would shake my hand and turn away because I wasn‚Äôt \"\n",
        "        \"worth talking to,‚Äù said Thrun, in an interview with Recode earlier \"\n",
        "        \"this week.\")\n",
        "doc = nlp(text)\n",
        "\n"
      ],
      "metadata": {
        "id": "TQu2XLLgJgcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "  print(token.text,token.pos_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCNACnVslYQF",
        "outputId": "048c9681-aa80-4579-acaf-9a7948936f87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When SCONJ\n",
            "Sebastian PROPN\n",
            "Thrun PROPN\n",
            "started VERB\n",
            "working VERB\n",
            "on ADP\n",
            "self NOUN\n",
            "- PUNCT\n",
            "driving VERB\n",
            "cars NOUN\n",
            "at ADP\n",
            "Google PROPN\n",
            "in ADP\n",
            "2007 NUM\n",
            ", PUNCT\n",
            "few ADJ\n",
            "people NOUN\n",
            "outside ADP\n",
            "of ADP\n",
            "the DET\n",
            "company NOUN\n",
            "took VERB\n",
            "him PRON\n",
            "seriously ADV\n",
            ". PUNCT\n",
            "‚Äú PUNCT\n",
            "I PRON\n",
            "can AUX\n",
            "tell VERB\n",
            "you PRON\n",
            "very ADV\n",
            "senior ADJ\n",
            "CEOs NOUN\n",
            "of ADP\n",
            "major ADJ\n",
            "American ADJ\n",
            "car NOUN\n",
            "companies NOUN\n",
            "would AUX\n",
            "shake VERB\n",
            "my PRON\n",
            "hand NOUN\n",
            "and CCONJ\n",
            "turn VERB\n",
            "away ADV\n",
            "because SCONJ\n",
            "I PRON\n",
            "was AUX\n",
            "n‚Äôt PART\n",
            "worth ADJ\n",
            "talking VERB\n",
            "to ADP\n",
            ", PUNCT\n",
            "‚Äù PUNCT\n",
            "said VERB\n",
            "Thrun PROPN\n",
            ", PUNCT\n",
            "in ADP\n",
            "an DET\n",
            "interview NOUN\n",
            "with ADP\n",
            "Recode PROPN\n",
            "earlier ADV\n",
            "this DET\n",
            "week NOUN\n",
            ". PUNCT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the helper function\n",
        "def show_ents(doc):\n",
        "    if doc.ents:\n",
        "        for ent in doc.ents:\n",
        "            print(ent.text, ent.label_, spacy.explain(ent.label_))\n",
        "    else:\n",
        "        print(\"No entities found.\")\n",
        "\n",
        "show_ents(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoKpvxQioCTg",
        "outputId": "291d6f1e-65aa-4d61-e78e-3edccf9395c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sebastian Thrun PERSON People, including fictional\n",
            "Google ORG Companies, agencies, institutions, etc.\n",
            "2007 DATE Absolute or relative dates or periods\n",
            "American NORP Nationalities or religious or political groups\n",
            "Thrun GPE Countries, cities, states\n",
            "Recode ORG Companies, agencies, institutions, etc.\n",
            "earlier this week DATE Absolute or relative dates or periods\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print token with POS and detailed tag\n",
        "for token in doc:\n",
        "    print(f\"{token.text:<12} {token.pos_:<10} {token.tag_:<10} {spacy.explain(token.tag_)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAEEV17P9KWy",
        "outputId": "d0692bf6-5469-4453-8ffd-8b5633c5eacb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When         SCONJ      WRB        wh-adverb\n",
            "Sebastian    PROPN      NNP        noun, proper singular\n",
            "Thrun        PROPN      NNP        noun, proper singular\n",
            "started      VERB       VBD        verb, past tense\n",
            "working      VERB       VBG        verb, gerund or present participle\n",
            "on           ADP        IN         conjunction, subordinating or preposition\n",
            "self         NOUN       NN         noun, singular or mass\n",
            "-            PUNCT      HYPH       punctuation mark, hyphen\n",
            "driving      VERB       VBG        verb, gerund or present participle\n",
            "cars         NOUN       NNS        noun, plural\n",
            "at           ADP        IN         conjunction, subordinating or preposition\n",
            "Google       PROPN      NNP        noun, proper singular\n",
            "in           ADP        IN         conjunction, subordinating or preposition\n",
            "2007         NUM        CD         cardinal number\n",
            ",            PUNCT      ,          punctuation mark, comma\n",
            "few          ADJ        JJ         adjective (English), other noun-modifier (Chinese)\n",
            "people       NOUN       NNS        noun, plural\n",
            "outside      ADP        IN         conjunction, subordinating or preposition\n",
            "of           ADP        IN         conjunction, subordinating or preposition\n",
            "the          DET        DT         determiner\n",
            "company      NOUN       NN         noun, singular or mass\n",
            "took         VERB       VBD        verb, past tense\n",
            "him          PRON       PRP        pronoun, personal\n",
            "seriously    ADV        RB         adverb\n",
            ".            PUNCT      .          punctuation mark, sentence closer\n",
            "‚Äú            PUNCT      ``         opening quotation mark\n",
            "I            PRON       PRP        pronoun, personal\n",
            "can          AUX        MD         verb, modal auxiliary\n",
            "tell         VERB       VB         verb, base form\n",
            "you          PRON       PRP        pronoun, personal\n",
            "very         ADV        RB         adverb\n",
            "senior       ADJ        JJ         adjective (English), other noun-modifier (Chinese)\n",
            "CEOs         NOUN       NNS        noun, plural\n",
            "of           ADP        IN         conjunction, subordinating or preposition\n",
            "major        ADJ        JJ         adjective (English), other noun-modifier (Chinese)\n",
            "American     ADJ        JJ         adjective (English), other noun-modifier (Chinese)\n",
            "car          NOUN       NN         noun, singular or mass\n",
            "companies    NOUN       NNS        noun, plural\n",
            "would        AUX        MD         verb, modal auxiliary\n",
            "shake        VERB       VB         verb, base form\n",
            "my           PRON       PRP$       pronoun, possessive\n",
            "hand         NOUN       NN         noun, singular or mass\n",
            "and          CCONJ      CC         conjunction, coordinating\n",
            "turn         VERB       VB         verb, base form\n",
            "away         ADV        RB         adverb\n",
            "because      SCONJ      IN         conjunction, subordinating or preposition\n",
            "I            PRON       PRP        pronoun, personal\n",
            "was          AUX        VBD        verb, past tense\n",
            "n‚Äôt          PART       RB         adverb\n",
            "worth        ADJ        JJ         adjective (English), other noun-modifier (Chinese)\n",
            "talking      VERB       VBG        verb, gerund or present participle\n",
            "to           ADP        IN         conjunction, subordinating or preposition\n",
            ",            PUNCT      ,          punctuation mark, comma\n",
            "‚Äù            PUNCT      ''         closing quotation mark\n",
            "said         VERB       VBD        verb, past tense\n",
            "Thrun        PROPN      NNP        noun, proper singular\n",
            ",            PUNCT      ,          punctuation mark, comma\n",
            "in           ADP        IN         conjunction, subordinating or preposition\n",
            "an           DET        DT         determiner\n",
            "interview    NOUN       NN         noun, singular or mass\n",
            "with         ADP        IN         conjunction, subordinating or preposition\n",
            "Recode       PROPN      NNP        noun, proper singular\n",
            "earlier      ADV        RBR        adverb, comparative\n",
            "this         DET        DT         determiner\n",
            "week         NOUN       NN         noun, singular or mass\n",
            ".            PUNCT      .          punctuation mark, sentence closer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NER"
      ],
      "metadata": {
        "id": "mise6JmYDsYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_, spacy.explain(ent.label_))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBS2XIEoBr54",
        "outputId": "f8177975-3a6b-4fac-9659-f36f58748350"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sebastian Thrun PERSON People, including fictional\n",
            "Google ORG Companies, agencies, institutions, etc.\n",
            "2007 DATE Absolute or relative dates or periods\n",
            "American NORP Nationalities or religious or political groups\n",
            "Thrun GPE Countries, cities, states\n",
            "Recode ORG Companies, agencies, institutions, etc.\n",
            "earlier this week DATE Absolute or relative dates or periods\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "displacy.render(doc,style = 'ent',jupyter = True)"
      ],
      "metadata": {
        "id": "np_WR5cKnbIj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "64fa7112-70d8-4a1e-bf88-d201e89b9463"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">When \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Sebastian Thrun\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " started working on self-driving cars at \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Google\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " in \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    2007\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ", few people outside of the company took him seriously. ‚ÄúI can tell you very senior CEOs of major \n",
              "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    American\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
              "</mark>\n",
              " car companies would shake my hand and turn away because I wasn‚Äôt worth talking to,‚Äù said \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Thrun\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ", in an interview with \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Recode\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    earlier this week\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ".</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "sentence segmentation"
      ],
      "metadata": {
        "id": "hFYKs3PDEobY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for sent in doc.sents:\n",
        "    print(sent.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRu7_NJTEo-Q",
        "outputId": "f59673da-6536-4557-8808-b170284da0a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When Sebastian Thrun started working on self-driving cars at Google in 2007, few people outside of the company took him seriously.\n",
            "‚ÄúI can tell you very senior CEOs of major American car companies would shake my hand and turn away because I wasn‚Äôt worth talking to,‚Äù said Thrun, in an interview with Recode earlier this week.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bag of Words (BoW)"
      ],
      "metadata": {
        "id": "jcLrXab3j_jK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter"
      ],
      "metadata": {
        "id": "gZZ1OAf7vXdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bow = Counter([token.text.lower() for token in doc if not token.is_stop and not token.is_punct])\n",
        "print(bow)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rg_vpI1bj-zq",
        "outputId": "7421467b-a2c6-4b7d-9b09-a7afd9ecf5e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'thrun': 2, 'sebastian': 1, 'started': 1, 'working': 1, 'self': 1, 'driving': 1, 'cars': 1, 'google': 1, '2007': 1, 'people': 1, 'outside': 1, 'company': 1, 'took': 1, 'seriously': 1, 'tell': 1, 'senior': 1, 'ceos': 1, 'major': 1, 'american': 1, 'car': 1, 'companies': 1, 'shake': 1, 'hand': 1, 'turn': 1, 'away': 1, 'worth': 1, 'talking': 1, 'said': 1, 'interview': 1, 'recode': 1, 'earlier': 1, 'week': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vocabulary\n",
        "vocab = list(bow.keys())\n",
        "\n",
        "# Create vector (counts for each vocab word)\n",
        "vector = [bow[word] for word in vocab]\n",
        "\n",
        "print(\"Vocabulary:\", vocab)\n",
        "print(\"Vector:\", vector)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-DX1CXrnAT7",
        "outputId": "5d1fc871-c871-4255-9f99-dfba8fc67171"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['sebastian', 'thrun', 'started', 'working', 'self', 'driving', 'cars', 'google', '2007', 'people', 'outside', 'company', 'took', 'seriously', 'tell', 'senior', 'ceos', 'major', 'american', 'car', 'companies', 'shake', 'hand', 'turn', 'away', 'worth', 'talking', 'said', 'interview', 'recode', 'earlier', 'week']\n",
            "Vector: [1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF/IDF"
      ],
      "metadata": {
        "id": "X26P4ELGmtzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Load spaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Example corpus\n",
        "corpus = [\n",
        "    \"Apple is looking at buying a U.K. startup.\",\n",
        "    \"Apple will invest $1 billion in the startup.\",\n",
        "    \"Google is launching a new product next year.\"\n",
        "]\n",
        "\n",
        "# Custom tokenizer using spaCy\n",
        "def spacy_tokenizer(sentence):\n",
        "    doc = nlp(sentence)\n",
        "    return [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
        "\n",
        "# Create TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer(tokenizer=spacy_tokenizer)\n",
        "\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(\"Feature names (vocabulary):\")\n",
        "print(vectorizer.get_feature_names_out())\n",
        "\n",
        "print(\"\\nTF-IDF Matrix:\")\n",
        "print(X.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3mPF2Famvtd",
        "outputId": "c682a327-dd7c-49f5-b45b-c87fe29eaa3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature names (vocabulary):\n",
            "['$' '1' 'apple' 'billion' 'buy' 'google' 'invest' 'launch' 'look' 'new'\n",
            " 'product' 'startup' 'u.k' 'year']\n",
            "\n",
            "TF-IDF Matrix:\n",
            "[[0.         0.         0.37302199 0.         0.49047908 0.\n",
            "  0.         0.         0.49047908 0.         0.         0.37302199\n",
            "  0.49047908 0.        ]\n",
            " [0.44036207 0.44036207 0.3349067  0.44036207 0.         0.\n",
            "  0.44036207 0.         0.         0.         0.         0.3349067\n",
            "  0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.4472136\n",
            "  0.         0.4472136  0.         0.4472136  0.4472136  0.\n",
            "  0.         0.4472136 ]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Class 3**"
      ],
      "metadata": {
        "id": "Ypt03Hj5LV_0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EMBEDDINGS**"
      ],
      "metadata": {
        "id": "YZQmj4G9JIyD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1Ô∏è‚É£ Word Embeddings range (‚Äì1 to 1?)\n",
        "\n",
        "In models like Word2Vec, GloVe, FastText, each word is mapped to a fixed-length vector (say 100, 200, or 300 dimensions).\n",
        "\n",
        "These numbers are not restricted to ‚Äì1 to 1. They can be any real numbers (e.g., ‚Äì5.2, 0.34, 2.7).\n",
        "\n",
        "But many implementations normalize vectors so cosine similarity is easier. After normalization, values may fall roughly between ‚Äì1 and 1.\n",
        "\n",
        "2Ô∏è‚É£ Sentence Embeddings range\n",
        "\n",
        "A sentence embedding is just a vector representation of a full sentence (instead of a single word).\n",
        "\n",
        "Like word embeddings, sentence embeddings also contain real-valued numbers, not strictly bounded.\n",
        "\n",
        "Depending on the model (e.g., Sentence-BERT), values can be anywhere in the real space (but often between ‚Äì1 and 1 after normalization).\n",
        "\n",
        "3Ô∏è‚É£ What are the ‚Äúcolumns‚Äù in an embedding?\n",
        "\n",
        "Think of an embedding as a matrix:\n",
        "\n",
        "Each row = a word (or a sentence/document).\n",
        "\n",
        "Each column (dimension) = a feature learned by the model."
      ],
      "metadata": {
        "id": "GpljaPjQlgyT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example of sentence embeddings**"
      ],
      "metadata": {
        "id": "Ghin3ubBmOQu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîé Step 1: Input Example Sentences\n",
        "\n",
        "Let‚Äôs take two pairs of sentences:\n",
        "\n",
        "Pair 1 (similar meaning):\n",
        "\n",
        "S1: ‚ÄúI love playing football.‚Äù\n",
        "\n",
        "S2: ‚ÄúI enjoy playing soccer.‚Äù\n",
        "\n",
        "Pair 2 (different meaning):\n",
        "\n",
        "S3: ‚ÄúThe sun is shining today.‚Äù\n",
        "\n",
        "S4: ‚ÄúI cooked pasta for dinner.‚Äù\n",
        "\n",
        "---\n",
        "\n",
        "üîé Step 2: How embeddings are generated\n",
        "Using a transformer model (e.g., Sentence-BERT):\n",
        "\n",
        "Tokenization:\n",
        "Each sentence is split into tokens (words or subwords).\n",
        "Example: ‚ÄúI love playing football‚Äù ‚Üí [CLS], I, love, playing, football, [SEP]\n",
        "\n",
        "Model Encoding:\n",
        "Tokens pass through a transformer (BERT-like). Each token gets a contextual vector (768D for BERT-base).\n",
        "\n",
        "Pooling:\n",
        "To create a single sentence embedding, we pool token embeddings:\n",
        "\n",
        "Often by taking the [CLS] token representation, or\n",
        "\n",
        "By averaging all token embeddings.\n",
        "\n",
        "üëâ Result: A fixed-length vector (e.g., 768 numbers) for the entire sentence.\n",
        "\n",
        "Example (numbers are illustrative, not real):\n",
        "\n",
        "\"I love playing football\" ‚Üí [0.12, -0.34, 0.87, ..., 0.45]   (768D)\n",
        "\"I enjoy playing soccer\"  ‚Üí [0.10, -0.30, 0.85, ..., 0.50]   (768D)\n",
        "\n",
        "---\n",
        "\n",
        "üîé Step 3: How similarity is checked\n",
        "\n",
        "The most common way = Cosine Similarity:\n",
        "\n",
        "Dot product of two vectors, divided by product of their magnitudes.\n",
        "\n",
        "Range = ‚Äì1 to 1.\n",
        "\n",
        "1 ‚Üí perfectly similar (same direction)\n",
        "\n",
        "0 ‚Üí unrelated (orthogonal)\n",
        "\n",
        "‚Äì1 ‚Üí opposite meaning\n",
        "\n",
        "---\n",
        "\n",
        "üîé Step 4: Example Results\n",
        "\n",
        "Suppose we compute cosine similarity:\n",
        "\n",
        "S1 vs S2 (football vs soccer) ‚Üí 0.92 ‚úÖ (high similarity)\n",
        "\n",
        "S1 vs S3 (football vs sun) ‚Üí 0.20 ‚ùå (low similarity)\n",
        "\n",
        "S3 vs S4 (sun vs pasta) ‚Üí 0.05 ‚ùå (almost unrelated)"
      ],
      "metadata": {
        "id": "rJfi6GuZlxAj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2 (How embeddings are generated)**"
      ],
      "metadata": {
        "id": "ShuDS1fQmY2J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ 1. Tokenization\n",
        "\n",
        "Transformer models (like BERT) can‚Äôt directly take raw text.\n",
        "\n",
        "They split text into tokens (words or subwords).\n",
        "\n",
        "Special tokens are added:\n",
        "\n",
        "[CLS] = signals start of sentence (used for classification / embedding).\n",
        "\n",
        "[SEP] = signals end of sentence.\n",
        "\n",
        "Example:\n",
        "\n",
        "\"I love playing football\"\n",
        "‚Üí [CLS], \"I\", \"love\", \"playing\", \"football\", [SEP]\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "üîπ 2. Model Encoding\n",
        "\n",
        "Each token is converted into a vector embedding (initially from a lookup table).\n",
        "\n",
        "These embeddings go through multiple transformer layers (self-attention, feed-forward).\n",
        "\n",
        "At the end, each token has a contextual vector (e.g., 768 dimensions for BERT-base).\n",
        "\n",
        "Example (shortened vectors, just for illustration):\n",
        "\n",
        "\"I\"        ‚Üí [0.12, -0.34, ..., 0.22]  \n",
        "\"love\"     ‚Üí [0.67,  0.10, ..., -0.45]  \n",
        "\"playing\"  ‚Üí [0.21,  0.98, ...,  0.11]  \n",
        "\"football\" ‚Üí [0.55, -0.29, ...,  0.90]  \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "üîπ 3. Pooling (combine into 1 sentence vector)\n",
        "\n",
        "Now we need one embedding for the entire sentence. Common methods:\n",
        "\n",
        "[CLS] token:\n",
        "Take the vector for [CLS] (trained to summarize the sentence).\n",
        "\n",
        "Mean pooling:\n",
        "Average all token embeddings (excluding [CLS] and [SEP]).\n",
        "\n",
        "Example (mean pooling, simplified to 3D just to see numbers):\n",
        "\n",
        "\"I\"        ‚Üí [0.1, -0.3, 0.2]  \n",
        "\"love\"     ‚Üí [0.6,  0.1, -0.4]  \n",
        "\"playing\"  ‚Üí [0.2,  1.0,  0.1]  \n",
        "\"football\" ‚Üí [0.5, -0.2, 0.9]  \n",
        "\n",
        "Mean = [(0.1+0.6+0.2+0.5)/4, (-0.3+0.1+1.0-0.2)/4, (0.2-0.4+0.1+0.9)/4]  \n",
        "     = [0.35, 0.15, 0.2]\n",
        "\n",
        "\n",
        "So the sentence embedding = [0.35, 0.15, 0.2] (in reality ‚Üí 768D or 1536D).\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "üîπ 4. Final Result\n",
        "\n",
        "Each sentence becomes a fixed-length vector (e.g., 768 numbers for BERT-base).\n",
        "\n",
        "Different sentences with similar meaning ‚Üí embeddings that are close together in vector space.\n",
        "\n",
        "‚úÖ So the pipeline is:\n",
        "Sentence ‚Üí Tokens ‚Üí Contextual embeddings for each token ‚Üí Pooling ‚Üí Sentence embedding (vector)."
      ],
      "metadata": {
        "id": "p2W3G0_TmY88"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "                                 ----\n",
        "                                 "
      ],
      "metadata": {
        "id": "yipapKMTtpym"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sentiment Analysis**\n",
        "\n",
        "\n",
        "For Sentiment Analysis NLTK is better than Spacy"
      ],
      "metadata": {
        "id": "_lAnWr1YtxU0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QM71U_5Ktwhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "soFMV5O4tp95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "GeVegatvLX1Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e91ab80a-d29e-431d-c3ff-ca22a0c2014d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kK_LeHX7RDc",
        "outputId": "01a2c335-2cc0-40a9-f3f6-2b6b26cf9648"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Initialize\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Example text\n",
        "text1 = \"I love this product! It's amazing and works perfectly.\"\n",
        "text2 = \"This is the worst experience I've ever had.\"\n",
        "\n",
        "# Get sentiment scores\n",
        "print(sia.polarity_scores(text1))\n",
        "print(sia.polarity_scores(text2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhG4fqV87TK6",
        "outputId": "d0150639-dfbf-4898-bc9b-38f462f4700e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'neg': 0.0, 'neu': 0.286, 'pos': 0.714, 'compound': 0.9259}\n",
            "{'neg': 0.369, 'neu': 0.631, 'pos': 0.0, 'compound': -0.6249}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "neg ‚Üí Negative sentiment score\n",
        "\n",
        "neu ‚Üí Neutral sentiment score\n",
        "\n",
        "pos ‚Üí Positive sentiment score\n",
        "\n",
        "compound ‚Üí Normalized, overall sentiment score (ranges from -1 to +1)\n",
        "\n",
        "üìå How to interpret compound score\n",
        "\n",
        "compound >= 0.05 ‚Üí Positive\n",
        "\n",
        "compound <= -0.05 ‚Üí Negative\n",
        "\n",
        "Otherwise ‚Üí Neutral"
      ],
      "metadata": {
        "id": "ayYUnxfd7rjy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üöÄ Let‚Äôs break down how **VADER (Valence Aware Dictionary for sEntiment Reasoning)** works internally. It‚Äôs a **lexicon + rule‚Äìbased sentiment analyzer** specially tuned for social media, reviews, and informal text.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è How VADER Works Internally\n",
        "\n",
        "### 1. **Sentiment Lexicon**\n",
        "\n",
        "* At its core, VADER has a **dictionary of \\~7,500 words/phrases**.\n",
        "* Each entry has a **valence score** ranging from **-4 (most negative)** to **+4 (most positive)**.\n",
        "* Examples:\n",
        "\n",
        "  * \"excellent\" ‚Üí +3.1\n",
        "  * \"horrible\" ‚Üí -2.5\n",
        "  * \"meh\" ‚Üí -0.2\n",
        "  * \"love\" ‚Üí +3.2\n",
        "  * \"hate\" ‚Üí -3.0\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Text Preprocessing**\n",
        "\n",
        "* Input text is tokenized (split into words, emoticons, punctuation).\n",
        "* VADER keeps emoticons, emojis, punctuation, and capitalization ‚Äî because they carry sentiment (unlike normal text cleaning).\n",
        "\n",
        "Example:\n",
        "\n",
        "* \"I LOVE it!!! üòçüòç\" ‚Üí tokens: \\[I, LOVE, it, !!!, üòç, üòç]\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Valence Scoring per Token**\n",
        "\n",
        "* Each token is checked against the lexicon.\n",
        "* If found, its score is retrieved.\n",
        "* If not found, score = 0.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Heuristic Adjustments (Rules)**\n",
        "\n",
        "VADER doesn‚Äôt just sum up words ‚Äî it applies **rules** to capture sentiment intensity:\n",
        "\n",
        "#### üîπ (a) **Degree modifiers (intensifiers & dampeners)**\n",
        "\n",
        "Words like \"very\", \"extremely\", \"slightly\" adjust sentiment strength.\n",
        "\n",
        "* \"very good\" ‚Üí \"good\" (+2) √ó 1.5 = +3.0\n",
        "* \"slightly bad\" ‚Üí \"bad\" (-2) √ó 0.5 = -1.0\n",
        "\n",
        "#### üîπ (b) **Negation handling**\n",
        "\n",
        "Words like \"not\", \"never\", \"isn't\" flip/soften the sentiment.\n",
        "\n",
        "* \"good\" ‚Üí +2\n",
        "* \"not good\" ‚Üí \\~ -1.5\n",
        "\n",
        "#### üîπ (c) **Punctuation**\n",
        "\n",
        "* Exclamation marks **boost intensity** (but capped).\n",
        "\n",
        "  * \"good\" ‚Üí +2\n",
        "  * \"good!!\" ‚Üí +2.5\n",
        "* Question marks sometimes increase emphasis.\n",
        "\n",
        "#### üîπ (d) **Capitalization**\n",
        "\n",
        "* UPPERCASE words are more intense.\n",
        "\n",
        "  * \"good\" ‚Üí +2\n",
        "  * \"GOOD\" ‚Üí +2.5\n",
        "\n",
        "#### üîπ (e) **Conjunction ‚Äúbut‚Äù**\n",
        "\n",
        "* Text after \"but\" gets more weight.\n",
        "\n",
        "  * \"The movie was boring, BUT the acting was great.\"\n",
        "    ‚Üí \"boring\" effect reduced, \"great\" boosted.\n",
        "\n",
        "#### üîπ (f) **Emoticons & emojis**\n",
        "\n",
        "* üòä (+2), üò° (-2.5), ‚ù§Ô∏è (+3), etc.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Aggregating Scores**\n",
        "\n",
        "* After adjusting, VADER sums the valence of all tokens.\n",
        "* It normalizes the score to a range between **-1 and +1** using this formula:\n",
        "\n",
        "$$\n",
        "compound = \\frac{sum\\_valence}{\\sqrt{sum\\_valence^2 + \\alpha}}\n",
        "$$\n",
        "\n",
        "where **Œ± = 15**, a normalization constant to keep values in (-1,1).\n",
        "\n",
        "---\n",
        "\n",
        "### 6. **Final Output**\n",
        "\n",
        "VADER returns a dictionary with **4 scores**:\n",
        "\n",
        "```python\n",
        "{'neg': 0.xx, 'neu': 0.xx, 'pos': 0.xx, 'compound': -0.6789}\n",
        "```\n",
        "\n",
        "* **pos/neu/neg** = proportion of text that falls into each sentiment.\n",
        "* **compound** = single normalized score in \\[-1,1].\n",
        "\n",
        "  * ‚â• 0.05 ‚Üí Positive\n",
        "  * ‚â§ -0.05 ‚Üí Negative\n",
        "  * Otherwise ‚Üí Neutral"
      ],
      "metadata": {
        "id": "bI02wqBsRhk9"
      }
    }
  ]
}